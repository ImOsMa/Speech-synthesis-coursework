# -*- coding: utf-8 -*-
"""PyTorch TTS

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZKoG9QlQ741NJjAoC38zAEr6n8peMADx

# Tacotron2

## Prepair for training

Mount Google Drive to store all checkpoints and datasets
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Download Dataset

Download LJ dataset
"""

# %cd /content/drive/MyDrive/
# !wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2
# !tar -xf LJSpeech-1.1.tar.bz2

"""Download M-AILABS Russian dataset"""

# %cd /content/drive/MyDrive/
# !wget http://www.caito.de/data/Training/stt_tts/ru_RU.tgz
# !tar -zxvf ru_RU.tgz
# # Rename directory
# !mv ru_RU M_AILABS_Ru

"""### Preprocess Dataset

"""

import random

# Commented out IPython magic to ensure Python compatibility.
"""
Move all wav file to the common folder
Concat all text files into one
"""
def refactor_M_AILABS(sex='female', speaker_index=0
                      , dataset_dir="/content/drive/MyDrive/M_AILABS_Ru/"
                      , out_dir="/content/drive/MyDrive/M_AILABS_Ru/"
                      , move_files=True
                      ):
  print(" *** Start refactoring... ***")
  path = f"{dataset_dit}{''if dataset_dit[-1] == '/' else '/'}by_book/{sex}"
#   %cd $path
  speakers = !ls
  books_path = f"{path}/{speakers[speaker_index]}"
#   %cd $books_path
  books_folders = !ls -d */ | cut -f1 -d'/'
  path = f"{out_dir}{''if out_dir[-1] == '/' else '/'}"
#   %cd $path
  !mkdir wavs
  wavs_path = f"{path}wavs/"
  !touch mlbs_audio_text.txt
  for book in books_folders:
    cur_wavs_path = f"{books_path}/{book}/wavs/*"
    cur_text_path = f"{books_path}/{book}/metadata.csv"
    !cat $cur_text_path >> mlbs_audio_text.txt
    if move_files:
      !mv -v $cur_wavs_path $wavs_path
  print(" *** Refactoring finished! ***")

# refactor_M_AILABS()

"""
M-Ailabs dataset has 2 strings of audio trascription -
raw and normalized
This function chooose one, based on argument 
and add wav file full path
"""
def transform_m_ailabs_text(file_name="mlbs_audio_text.txt",
                            new_file_name="mlbs_audio_text_transformed.txt",
                            dataset_dir="/content/drive/MyDrive/M_AILABS_Ru/",
                            raw_data=False):
  write_file = open(dataset_dir+new_file_name, 'w', encoding='utf-8')
  with open(dataset_dir+file_name, 'r', encoding='utf-8') as read_file:
    for line in read_file:
      line = line.split('|')
      wav_file_path = f"{dataset_dir}wavs/{line[0]}.wav"
      new_line = f"{wav_file_path}|{line[1] if raw_data else line[2]}"
      write_file.write(new_line)

# transform_m_ailabs_text()

"""
We should split data into 3 sets:
'train', 'test' and 'eval'
"""
def split_dataset(test_size=0.05, eval_size=0.01,
                  dataset_dir="/content/drive/MyDrive/M_AILABS_Ru/",
                  source_file_name="mlbs_audio_text_transformed.txt",
                  output_file_name_base="mlbs_audio_text",
                  shuffle=True):
  if test_size + eval_size >= 1:
    print("ERROR: (test_size + eval_size) must be less 1")
    return
  
  data = []
  with open(f"{dataset_dir}{source_file_name}",
            'r', encoding='utf-8') as read_file:
    for line in read_file:
      data.append(line)
  
  length = len(data)
  test_length = int(length * test_size)
  eval_length = int(length * eval_size)
  if shuffle:
    random.shuffle(data)
  test_data = data[: test_length]
  eval_data = data[test_length : test_length + eval_length]
  train_data = data[test_length + eval_length :]

  sets = {'test': test_data, 'eval': eval_data, 'train': train_data}
  for name, dataset in sets.items():
    with open(f"{dataset_dir}{output_file_name_base}_{name}.txt", 
              'w', encoding='utf-8') as file:
      for line in dataset:
        file.write(line)

# split_dataset(shuffle=False)

"""### Install libs"""

# !git clone https://github.com/NVIDIA/tacotron2.git

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/lightbooster/tacotron2.git
# %cd tacotron2
!git checkout russian-cleaners

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/tacotron2/

!git submodule init; git submodule update

# %cd ..
# !git clone https://github.com/NVIDIA/apex
# %cd apex
# !pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
# %cd ../tacotron2

!cat requirements.txt

!pip install apex
!pip install Unidecode
!pip install tensorflow==1.15.2

"""Change the dirrectory of wav files (for LJ dataset)"""

# !sed -i -- 's,DUMMY,../drive/MyDrive/LJSpeech-1.1/wavs,g' filelists/*.txt

"""## Train"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/tacotron2/

"""### Hyperparams"""

home_dir = "/content/drive/"

training_files= home_dir + 'MyDrive/M_AILABS_Ru/mlbs_audio_text_train.txt'
validation_files= home_dir + 'MyDrive/M_AILABS_Ru/mlbs_audio_text_eval.txt'

output_directory = home_dir + "MyDrive/M_AILABS_Ru/checkpoints"
log_directory = "../logs"

batch_size = 16
iters_per_checkpoint = 600
sampling_rate = 16000

hparams = f"batch_size={batch_size},\
            iters_per_checkpoint={iters_per_checkpoint},\
            training_files={training_files},\
            validation_files={validation_files},\
            sampling_rate={sampling_rate}"

hparams

"""### Pretrained weights

NVIDEA checkpoint and deleted language weights
"""

# checkpoint_path = "../drive/MyDrive/tacotron2_statedict.pt"
# !python train.py --output_directory=$output_directory --log_directory=$log_directory --hparams "$hparams" -c $checkpoint_path --warm_start

"""my checkpoint and saved weights"""

checkpoint_num = 38400
checkpoint_path = f"/content/drive/MyDrive/M_AILABS_Ru/checkpoints/checkpoint_{checkpoint_num}"
!python train.py --output_directory=$output_directory --log_directory=$log_directory --hparams "$hparams" -c $checkpoint_path

"""### From the start"""

# !python train.py --output_directory="../logs" --hparams "batch_size=16, iters_per_checkpoint=300" --checkpoint_path "../drive/MyDrive/LJSpeech-1.1/checkpoints/checkpoint_100"

"""## Eval"""

import matplotlib
import matplotlib.pylab as plt

import IPython.display as ipd

import sys
sys.path.append('waveglow/')
import numpy as np
import torch

from hparams import create_hparams
from model import Tacotron2
from layers import TacotronSTFT, STFT
from audio_processing import griffin_lim
from train import load_model
from text import text_to_sequence
from denoiser import Denoiser

def plot_data(data, figsize=(16, 4)):
    fig, axes = plt.subplots(1, len(data), figsize=figsize)
    for i in range(len(data)):
        axes[i].imshow(data[i], aspect='auto', origin='bottom', 
                       interpolation='none')

"""### WaveGlow"""

waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow', pretrained=False)
checkpoint = torch.hub.load_state_dict_from_url('https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427')

# # Unwrap the DistributedDataParallel module
state_dict = {key.replace("module.", ""): value for key, value in checkpoint["state_dict"].items()}

# # Apply the state dict to the model
waveglow.load_state_dict(state_dict)
waveglow = waveglow.remove_weightnorm(waveglow)
waveglow = waveglow.to('cuda')
_ = waveglow.eval()

"""### Tacotron2"""

# tacotron2 checkpoint url
# https://api.ngc.nvidia.com/v2/models/nvidia/tacotron2_pyt_ckpt_fp32/versions/19.09.0/files/nvidia_tacotron2pyt_fp32_20190427

checkpoint_dir = "../drive/MyDrive/M_AILABS_Ru/checkpoints/"
checkpoint_name = "checkpoint_"
checkpoint_num = 18600
checkpoint_path = checkpoint_dir + checkpoint_name + str(checkpoint_num)
checkpoint_path

hparams = create_hparams()
hparams.sampling_rate = 16000
hparams.gate_threshold = 0.5

model = load_model(hparams)
model.load_state_dict(torch.load(checkpoint_path)['state_dict'])
_ = model.cuda().eval()

"""## Testing

### Load Mel spectogram from .npy file
"""

mel_numpy = np.load('../mel-batch.npy')
mel_numpy = np.array([mel_numpy.T])
mel_numpy.shape

cuda0 = torch.device('cuda:0')
loaded_mel = torch.tensor(mel_numpy, device=cuda0)
loaded_mel.shape

loaded_mel

plt.imshow(loaded_mel.float().data.cpu().numpy()[0], 
       aspect='auto', origin='bottom', interpolation='none')
plt.show()

"""### Inference"""

text = "это мой текст, который я написал."

# preprocessing
sequence = np.array(text_to_sequence(text, ['russian_cleaners']))[None, :]
sequence = torch.from_numpy(sequence).to(device='cuda', dtype=torch.int64)
sequence

# run the models
mel_outputs = mel_outputs_postnet = alignments = None
with torch.no_grad():
    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)
    audio = waveglow.infer(mel_outputs_postnet)
audio_numpy = audio[0].data.cpu().numpy()
rate = 16000
mel_outputs.shape

mel_outputs

"""Visualize Mels"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plot_data((mel_outputs.float().data.cpu().numpy()[0],
           mel_outputs_postnet.float().data.cpu().numpy()[0],
           alignments.float().data.cpu().numpy()[0].T))
plt.show()

"""### Audio"""

with torch.no_grad():
    audio = waveglow.infer(mel_outputs)
audio_numpy = audio[0].data.cpu().numpy()
rate = 16000
ipd.Audio(audio_numpy, rate=rate)

"""Smooth"""

with torch.no_grad():
    audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)
ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)

"""Griffin - Lim"""

import librosa
mel_gl = mel_outputs.cpu().numpy()[0]
gl_audio = librosa.griffinlim(mel_gl)
print(type(gl_audio))
ipd.Audio(gl_audio, rate=rate)